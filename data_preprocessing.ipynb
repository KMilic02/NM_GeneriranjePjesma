{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b36d98b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"<UNK>\": 1,\n",
    "    \"<SOS>\": 2,\n",
    "    \"<EOS>\": 3,\n",
    "    \"<LINE>\": 4, # represents the newline symbol at the end of the line\n",
    "    \"<STANZA>\": 5, # represents the double newline symbol that marks the beginnning of a new verse\n",
    "}\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35650abe",
   "metadata": {},
   "source": [
    "#### 1. Dataset\n",
    "\n",
    "### https://www.kaggle.com/datasets/carlosgdcj/genius-song-lyrics-with-language-information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df81027",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_csv = pd.read_csv(\"song_lyrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8747eb",
   "metadata": {},
   "source": [
    "#### 2. Dataset Preprocessing \n",
    "\n",
    "Genius songs are written in a format that needs some pre processing. Song metadata is often present between square brackets in the middle of the lyrics and the overall structure of the lyrics is preserved, meaning that each entry most likely contains a lot of new line characters that can cause some challenges when reading the data or passing it to a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835813d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lyrics(text):\n",
    "    text = re.sub(r\"\\[.*?\\]\", \" \", text)\n",
    "    text = re.sub(r\"\\(.*?\\)\", \" \", text)\n",
    "    text = text.replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"\\n{2,}\", \" <STANZA> \", text) # new verse break\n",
    "    text = re.sub(r\"\\n\", \" <LINE> \", text) # single line break\n",
    "    text = re.sub(r\"[^a-zA-Z0-9'?!.,<>\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "dataset_csv[\"year\"] = pd.to_numeric(dataset_csv[\"year\"], errors=\"coerce\")\n",
    "\n",
    "is_english = (\n",
    "    dataset_csv[\"language\"].eq(\"en\") |\n",
    "    (\n",
    "        dataset_csv[\"language\"].isna() &\n",
    "        dataset_csv[\"language_ft\"].eq(\"en\")\n",
    "    ) |\n",
    "    (\n",
    "        dataset_csv[\"language\"].isna() &\n",
    "        dataset_csv[\"language_cld3\"].eq(\"en\")\n",
    "    )\n",
    ")\n",
    "\n",
    "en = dataset_csv[is_english]\n",
    "\n",
    "subset = (\n",
    "    en[en[\"tag\"].isin([\"rap\"])]\n",
    "    .dropna(subset=[\"lyrics\", \"year\"])\n",
    "    .query(\"1980 <= year <= 2010\")\n",
    "    .drop_duplicates(subset=[\"title\", \"artist\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "subset[\"lyrics\"] = subset[\"lyrics\"].map(clean_lyrics)\n",
    "subset.to_csv(\"rap_song_lyrics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff69655",
   "metadata": {},
   "source": [
    "#### 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cdb8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"rap_song_lyrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "565ab9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, frequencies, max_size=-1, min_freq=0):\n",
    "        self.max_size = max_size\n",
    "        self.min_freq = min_freq\n",
    "        self.frequencies = frequencies\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "        for token in SPECIAL_TOKENS:\n",
    "            self._add_token(token)\n",
    "\n",
    "        for token, freq in sorted(frequencies.items(), key=lambda x: x[1], reverse=True):\n",
    "            if token in self.stoi:\n",
    "                continue # already added as special\n",
    "            if freq < min_freq:\n",
    "                continue\n",
    "            if 0 < max_size <= len(self.stoi):\n",
    "                break\n",
    "            self._add_token(token)\n",
    "\n",
    "    def _add_token(self, token):\n",
    "        idx = len(self.stoi)\n",
    "        self.stoi[token] = idx\n",
    "        self.itos[idx] = token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        if isinstance(tokens, str):\n",
    "            return torch.tensor(self.stoi.get(tokens, self.stoi[\"<UNK>\"]))\n",
    "        return torch.tensor([self.stoi.get(tok, self.stoi[\"<UNK>\"]) for tok in tokens])\n",
    "\n",
    "def build_vocab(token_lists, max_size=-1, min_freq=5):\n",
    "    freqs = Counter()\n",
    "    for tokens in token_lists:\n",
    "        freqs.update(tokens)\n",
    "    return Vocab(freqs, max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "def df_to_token_lists(frame):\n",
    "    return [[\"<SOS>\", *lyric.split(), \"<EOS>\"] for lyric in frame[\"lyrics\"]]\n",
    "\n",
    "# TRAIN/VAL/TEST - 80/10/10\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_tokens = df_to_token_lists(train_df)\n",
    "val_tokens = df_to_token_lists(val_df)\n",
    "test_tokens = df_to_token_lists(test_df)\n",
    "\n",
    "vocab = build_vocab(train_tokens, min_freq=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56a45248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398518\n",
      "42744\n",
      "0.022593112331188017\n"
     ]
    }
   ],
   "source": [
    "# Should we use subword tokens?\n",
    "\n",
    "freqs = Counter()\n",
    "for lyric in df[\"lyrics\"]:\n",
    "    freqs.update(lyric.split())\n",
    "print(len(freqs))\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "unk_id = vocab.stoi[\"<UNK>\"]\n",
    "unk_count = total = 0\n",
    "for tokens in val_tokens:\n",
    "    ids = vocab.encode(tokens)\n",
    "    unk_count += (ids == unk_id).sum().item()\n",
    "    total += len(ids)\n",
    "unk_ratio = unk_count / total\n",
    "print(unk_ratio)\n",
    "\n",
    "# It looks like the answer is probably not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5fad01",
   "metadata": {},
   "source": [
    "#### 4. LyricsDataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dbf7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, token_lists, vocab, seq_len=100, stride=1):\n",
    "        \"\"\"\n",
    "        token_lists: list of tokenized lyrics (each already includes <SOS>/<LINE>/<STANZA>/<EOS>)\n",
    "        vocab: Vocab instance\n",
    "        seq_len: number of input tokens per sample\n",
    "        \"\"\"\n",
    "        self.seq_len = seq_len\n",
    "        self.encoded_lyrics = [vocab.encode(tokens) for tokens in token_lists]\n",
    "        self.index = []\n",
    "        for song_idx, encoded in enumerate(self.encoded_lyrics):\n",
    "            max_start = len(encoded) - seq_len - 1\n",
    "            for start in range(0, max_start + 1, stride):\n",
    "                self.index.append((song_idx, start))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        song_idx, start = self.index[idx]\n",
    "        seq = self.encoded_lyrics[song_idx]\n",
    "        inputs = seq[start : start + self.seq_len]\n",
    "        targets = seq[start + 1 : start + self.seq_len + 1]\n",
    "        return inputs, targets\n",
    "\n",
    "train_dataset = LyricsDataset(train_tokens, vocab, seq_len=100)\n",
    "val_dataset = LyricsDataset(val_tokens, vocab, seq_len=100)\n",
    "test_dataset = LyricsDataset(test_tokens, vocab, seq_len=100)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c831b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
